{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem de tópicos com dados do Diário Oficial do Recife\n",
    "\n",
    "Neste notebook, construiremos um modelo de tópico usando o LdaModel da biblioteca Gensim e visualizaremos os resultados com ajuda da biblioteca Plotly.\n",
    "\n",
    "## Importar bibliotecas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.extend(['janeiro','fevereiro','março','abril','maio','junho','julho','agosto','setembro','outubro','novembro','dezembro','recife'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar o dataset\n",
    "\n",
    "Utilizaremos a biblioteca Pandas para importar o arquivo \"dore_2009_proc.csv\", uma coleção de documentos extraídos do Diário Oficial do Recife. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               Poder Executivo\n",
      "1               Poder Executivo\n",
      "2               Poder Executivo\n",
      "3               Poder Executivo\n",
      "4               Poder Executivo\n",
      "                 ...           \n",
      "5139    Poder Legislativo - old\n",
      "5140                   Notícias\n",
      "5141                   Notícias\n",
      "5142                   Notícias\n",
      "5143                   Notícias\n",
      "Name: secao, Length: 5144, dtype: object\n",
      "0       LEI Nº 17.597 /2009  EMENTA: DISPÕE SOBRE A SE...\n",
      "1       DECRETO Nº 25.024 DE 30 DE DEZEMBRODE 2009  EM...\n",
      "2       PORTARIA Nº 3130 DE 30 DE DEZEMBRO DE  2009  O...\n",
      "3       º 18012078.    PORTARIA Nº 3137 DE 30 DE DEZEM...\n",
      "4       esenvolvimento Urbano e Ambiental, a contar da...\n",
      "                              ...                        \n",
      "5139     dias do mês de novembro do ano corrente. E eu...\n",
      "5140    Opresidente da República, Luiz Inácio Lula da ...\n",
      "5141    O prefeito João Paulo anunciou, na última quar...\n",
      "5142    Em um dos seus últimos atos à frente da gestão...\n",
      "5143    Na última quarta-feira (31), o prefeito João P...\n",
      "Name: conteudo, Length: 5144, dtype: object\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "5139    10\n",
      "5140    11\n",
      "5141    11\n",
      "5142    11\n",
      "5143    11\n",
      "Name: int_label, Length: 5144, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "dataset = pd.read_csv(\"dore_2009_proc.csv\")\n",
    "dsecao = dataset[\"secao\"]\n",
    "dtext = dataset[\"conteudo\"]\n",
    "dlabel = dataset[\"int_label\"]\n",
    "print(dsecao)\n",
    "print(dtext)  \n",
    "print(dlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpar e preparar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lei', 'nº', 'ementa', 'dispoe', 'sobre', 'semana', 'municipal', 'de', 'cultura', 'evangelica', 'da', 'outras', 'providencias', 'faco', 'saber', 'que', 'poder', 'legislativo', 'do', 'municipio', 'rejeitou', 'veto', 'total', 'ao', 'projeto', 'de', 'lei', 'nº', 'do', 'poder', 'legislativo', 'eu', 'prefeito', 'da', 'cidade', 'do', 'recife', 'nos', 'termos', 'do', 'art', 'da', 'lei', 'organica', 'do', 'municipio', 'promulgo', 'seguinte', 'lei', 'art', 'fica', 'instituida', 'semana', 'municipal', 'de', 'cultura', 'evangelica', 'no', 'municipio', 'do', 'recife', 'paragrafo', 'unico', 'as', 'comemoracoes', 'ocorrerao', 'entre', 'os', 'dias', 'seis', 'doze', 'de', 'dezembro', 'art', 'tema', 'reflexoes', 'de', 'que', 'tratam', 'paragrafo', 'unico', 'entre', 'outros', 'abordarao', 'temas', 'como', 'educacao', 'religiosa', 'mercado', 'de', 'trabalho', 'saude', 'auto', 'estima', 'crianca', 'adolescente', 'relacao', 'com', 'sociedade', 'historia', 'luta', 'da', 'igreja', 'evangelica', 'liderancas', 'evangelicas', 'direitos', 'humanos', 'idosos', 'art', 'as', 'atividades', 'que', 'tratam', 'caput', 'do', 'artigo', 'primeiro', 'refere', 'se', 'manifestacoes', 'artisticas', 'culturais', 'debates', 'reflexoes', 'simposio', 'palestras', 'em', 'instituicoes', 'de', 'ensino', 'apresentacoes', 'artisticas', 'em', 'pracas', 'publicas', 'bem', 'como', 'outras', 'atividades', 'sobre', 'populacao', 'evangelica', 'no', 'brasil', 'estado', 'de', 'pernambuco', 'no', 'municipio', 'do', 'recife', 'art', 'semana', 'municipal', 'de', 'cultura', 'evangelica', 'no', 'municipio', 'do', 'recife', 'passa', 'fazer', 'parte', 'do', 'calendario', 'da', 'secretaria', 'de', 'cultura', 'do', 'recife', 'art', 'secretaria', 'de', 'cultura', 'do', 'municipio', 'do', 'recife', 'realizara', 'em', 'conjunto', 'com', 'as', 'secretarias', 'municipais', 'que', 'assim', 'achar', 'necessario', 'entidades', 'ligadas', 'ao', 'movimento', 'evangelico', 'no', 'municipio', 'do', 'recife', 'atividades', 'voltadas', 'para', 'realizacao', 'da', 'semana', 'municipal', 'de', 'cultura', 'evangelica', 'art', 'esta', 'lei', 'entrara', 'em', 'vigor', 'na', 'data', 'de', 'sua', 'publicacao', 'recife', 'de', 'dezembro', 'de', 'joao', 'da', 'costa', 'bezerra', 'filho', 'prefeito', 'do', 'recife', 'projeto', 'de', 'lei', 'nº', 'vereador', 'luiz', 'eustaquio', 'lei', 'nº', 'ementa', 'altera', 'art', 'caput', 'da', 'lei', 'municipal', 'nº', 'de', 'de', 'julho', 'de', 'povo', 'da', 'cidade', 'do', 'recife', 'por', 'seus', 'representantes', 'decretou', 'eu', 'em', 'seu', 'nome', 'sanciono', 'seguinte', 'lei', 'art', 'art', 'caput', 'da', 'lei', 'de', 'de', 'julho', 'de', 'passam', 'vigorar', 'com', 'seguinte', 'redacao', 'art', 'fica', 'instituido', 'abono', 'de', 'natureza', 'indenizatoria', 'destinado', 'aquisicao', 'de', 'computadores', 'ser', 'concedido', 'exclusivamente', 'aos', 'ocupantes', 'do', 'grupo', 'ocupacional', 'magisterio', 'gom', 'da', 'rede', 'de', 'ensino', 'publico', 'da', 'prefeitura', 'do', 'recife', 'que', 'estiverem', 'em', 'efetivo', 'exercicio', 'de', 'suas', 'funcoes', 'no', 'ambito', 'da', 'secretaria', 'de', 'educacao', 'esporte', 'lazer', 'os', 'ocupantes', 'do', 'grupo', 'ocupacional', 'magisterio', 'que', 'por', 'qualquer', 'motivo', 'encontrem', 'se', 'afastados', 'de', 'suas', 'funcoes', 'no', 'ambito', 'da', 'secretaria', 'de', 'educacao', 'esporte', 'lazer', 'so', 'farao', 'jus', 'ao', 'beneficio', 'de', 'que', 'trata', 'caput', 'quando', 'retornarem', 'ao', 'efetivo', 'exercicio', 'de', 'suas', 'atividades', 'nao', 'farao', 'jus', 'ao', 'abono', 'de', 'que', 'trata', 'caput', 'deste', 'artigo', 'os', 'ocupantes', 'do', 'grupo', 'ocupacional', 'magisterio', 'ja', 'contemplados', 'com', 'beneficios', 'semelhantes', 'no', 'ambito', 'dos', 'governos', 'estadual', 'ou', 'federal', 'art', 'esta', 'lei', 'entrara', 'em', 'vigor', 'na', 'data', 'de', 'sua', 'publicacao', 'recife', 'de', 'dezembro', 'de', 'joao', 'da', 'costa', 'bezerra', 'filho', 'prefeito', 'do', 'recife', 'projeto', 'de', 'lei', 'nº', 'de', 'autoria', 'poder', 'executivo']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = dtext\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Bigram, Trigram Models and Lemmatize\n",
    "\n",
    "Let’s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n",
    "\n",
    "Next, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n",
    "\n",
    "We keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, I use spacy for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('pt_core_news_sm')\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando a quantidade ideal de clusters (neste caso tópicos)\n",
    "\n",
    "Utilizaremos o TfidfVectorizer do Sklearn para a extração de palavras-chave, elas serão utilizadas para formar os clusters. Em seguida, calculamos o Sum of Squared Error (SSE) para vários tamanhos de cluster, neste caso até 14, e depois procuramos o \"cotovelo\" onde o SSE começa a se estabilizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.95,\n",
    "    max_features = 8000,\n",
    "    stop_words = stop_words\n",
    ")\n",
    "tfidf.fit(dtext)\n",
    "text = tfidf.transform(dtext)\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(text, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo o modelo de tópico\n",
    "\n",
    "Para criar o modelo de tópico LDA usando LdaModel(), precisamos do corpus e do dicionário. Primeiro iremos criá-los e depois construiremos o modelo. Os tópicos treinados (palavras-chave e pesos) também são impressos abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tópicos dominante e porcentagem de contribuição em cada documento\n",
    "\n",
    "Nos modelos de LDA, cada documento é composto de vários tópicos. Mas, normalmente, apenas um dos tópicos é o dominante. O código abaixo extrai esse tópico dominante para cada documento e mostra o peso do tópico e das palavras-chave em uma tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(682)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds com as principais palavras-chave em cada tópico\n",
    "\n",
    "Word Clouds em que o tamanho das palavras é proporcional ao peso delas em cada tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=2500,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE Clustering\n",
    "\n",
    "Agora visualizaremos os clusters de cada tópico em um espaço 2D usando o algoritmo t-SNE (t-distributed stochastic neighbor embedding) e a biblioteca Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "#arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layout 1\n",
    "\n",
    "(Sem cores para distinguir os pontos e com os tópicos dominantes que foram gerados na função format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready), implementada mais no início deste notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "n_topics = 20\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "fig1 = px.scatter(df, x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[dlabel], hover_data=[keywords[\"Dominant_Topic\"]], labels={'hover_data_0':'Dominant Topic'})\n",
    "fig1.show()\n",
    "\n",
    "#, color=mycolors[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dominant_topic = np.array(keywords[\"Dominant_Topic\"].values.tolist())\n",
    "\n",
    "array_dominant_topic = array_dominant_topic.astype(int)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "fig1 = px.scatter(df, x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[array_dominant_topic],  hover_data=[keywords[\"Dominant_Topic\"]], labels={'hover_data_0':'Dominant Topic'})\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.scatter(df, x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[array_dominant_topic],  hover_data=[dlabel], labels={'hover_data_0':'Label'})\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "colors = np.array(plt.cm.tab10(np.arange(20))) # + [\"crimson\", \"indigo\"]\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "fig2 = px.scatter(df, x=tsne_lda[:,0], y=tsne_lda[:,1], color=colors[dlabel], hover_data=[keywords[\"Dominant_Topic\"]], labels={'hover_data_0':'Dominant Topic'})\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "We started from scratch by importing, cleaning and processing the dataset to build the LDA model. Then we saw multiple ways to visualize the outputs of topic models including the word clouds and sentence coloring, which intuitively tells you what topic is dominant in each document. A t-SNE clustering and the pyLDAVis are used to provide more details into the clustering of the topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
